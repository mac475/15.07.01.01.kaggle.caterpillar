{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Cost에 log를 적용하는 이유는 뭔지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 R"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Reading in the data\n",
    "Train <- read.csv(\"train_set.csv\")\n",
    "Test <- read.csv(\"test_set.csv\")\n",
    "\n",
    "#Converting the quote_date field to a date columns\n",
    "str(Train)\n",
    "Train$qDate <- as.Date(Train$quote_date)\n",
    "Test$qDate <- as.Date(Test$quote_date)\n",
    "\n",
    "#Extracting year, month and weekday from quote_date\n",
    "library(lubridate)\n",
    "Train$qYear <- year(Train$qDate)\n",
    "Train$qMonth <- as.factor(month(Train$qDate))\n",
    "Train$qWeekDay <- as.factor(wday(Train$qDate))\n",
    "Train$lnCost <- log(Train$cost)\n",
    "\n",
    "Test$qYear <- year(Test$qDate)\n",
    "Test$qMonth <- as.factor(month(Test$qDate))\n",
    "Test$qWeekDay <- as.factor(wday(Test$qDate))\n",
    "\n",
    "#Training the model with 300 trees\n",
    "library(randomForest)\n",
    "randFrst <- randomForest(lnCost ~ annual_usage + min_order_quantity + bracket_pricing + quantity + qYear + qMonth, data = Train, nTree = 300)\n",
    "\n",
    "#Model Prediction\n",
    "PredictRF <- cbind(1:nrow(Test), exp(predict(randFrst, newdata = Test)))\n",
    "colnames(PredictRF) <- c('id', 'cost')\n",
    "write.csv(PredictRF, \"PredictRF.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Beating the Benchmark \n",
    "Caterpillar @ Kaggle\n",
    "\n",
    "__author__ : Abhishek\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "\n",
    "# load training and test datasets\n",
    "train = pd.read_csv('data/train_set.csv', parse_dates=[2,])\n",
    "test = pd.read_csv('data/test_set.csv', parse_dates=[3,])\n",
    "\n",
    "# create some new features\n",
    "train['year'] = train.quote_date.dt.year\n",
    "train['month'] = train.quote_date.dt.month\n",
    "train['dayofyear'] = train.quote_date.dt.dayofyear\n",
    "train['dayofweek'] = train.quote_date.dt.dayofweek\n",
    "train['day'] = train.quote_date.dt.day\n",
    "\n",
    "test['year'] = test.quote_date.dt.year\n",
    "test['month'] = test.quote_date.dt.month\n",
    "test['dayofyear'] = test.quote_date.dt.dayofyear\n",
    "test['dayofweek'] = test.quote_date.dt.dayofweek\n",
    "test['day'] = test.quote_date.dt.day\n",
    "\n",
    "\n",
    "# drop useless columns and create labels\n",
    "test = test.drop(['id', 'quote_date'], axis = 1)\n",
    "labels = train.cost.values\n",
    "train = train.drop(['quote_date', 'cost'], axis = 1)\n",
    "\n",
    "\n",
    "# convert data to numpy array\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "print train.shape, test.shape\n",
    "\n",
    "# label encode the categorical variables\n",
    "for i in range(train.shape[1]):\n",
    "    if i in [0,1,4]:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train[:,i]) + list(test[:,i]))\n",
    "        train[:,i] = lbl.transform(train[:,i])\n",
    "        test[:,i] = lbl.transform(test[:,i])\n",
    "\n",
    "\n",
    "# object array to float\n",
    "train = train.astype(float)\n",
    "test = test.astype(float)\n",
    "\n",
    "# i like to train on log(1+x) for RMSLE ;) \n",
    "# The choice is yours :)\n",
    "label_log = np.log1p(labels)\n",
    "\n",
    "# fit a random forest model\n",
    "clf = ensemble.RandomForestRegressor(n_jobs=-1, n_estimators=1000, random_state=42)\n",
    "clf.fit(train, label_log)\n",
    "\n",
    "# get predictions from the model, convert them and dump them!\n",
    "preds = np.expm1(clf.predict(test))\n",
    "sample = pd.read_csv('data/sample_submission.csv')\n",
    "sample['cost'] = preds\n",
    "sample.to_csv('benchmark.csv', index=False)\n",
    "\n",
    "# Swipe right on tinder ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Dataset 이슈들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- components.csv에 4개 elements가 존재하는 이슈 Row 있음 : 해결됨\n",
    "- comp_other.csv에 문제있다는데 확인되지 않음\n",
    "- comp_threaded.csv내 normal_size_1에 See drawing 있음 : 확인필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Keras : deep learning\n",
    "\n",
    "http://keras.io/#keras-theano-based-deep-learning-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. RMSLE 기반의 Grid Search CV 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle : RMSLE score of 1.68537: How to interpret ??](https://www.kaggle.com/forums/f/15/kaggle-forum/t/9933/rmsle-score-of-1-68537-how-to-interpret)\n",
    "\n",
    "[stackoverflow : RMSLE 최적화](http://stackoverflow.com/questions/27753775/gbrt-hyperparameter-tuning-using-gridsearchcv)\n",
    "\n",
    "##Question\n",
    "I'm running GridSearchCV to find the best parameters for GradientBoostingRegressor.\n",
    "The tutorial given was to use MSE for scoring.\n",
    "\n",
    "gs_cv = GridSearchCV(est, param_grid, scoring='mean_squared_error', n_jobs=4).fit(X_train, y_train)\n",
    "Is it possible to use other own defined scoring such as Root Mean Squared Logarithmic Error (RMSLE) to get the best hyperparameters?\n",
    "\n",
    "def rmsle(predicted, actual, size):\n",
    "    return np.sqrt(np.nansum(np.square(np.log(predicted + 1) - np.log(actual + 1)))/float(size))\n",
    "\n",
    "\n",
    "##Answer\n",
    "You need to make a custom scorer. In your case it would look like this:\n",
    "\n",
    "<pre>\n",
    "    from sklearn.metrics import make_scorer\n",
    "\n",
    "    scorer = make_scorer(rmsle, greater_is_better=False, size=10)\n",
    "    grid = GridSearchCV(est, param_grid, scoring=scorer)\n",
    "    \n",
    "</pre>\n",
    "\n",
    "shareedit\n",
    "answered Jan 3 at 14:47\n",
    "\n",
    "elyase\n",
    "12.2k11539\n",
    "  \t \t\n",
    "Awesome, thanks! –  ananuc Jan 5 at 10:37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. tube processing 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tube = pd.read_csv('../input/tube.csv',\n",
    "                       index_col=0,\n",
    "                       true_values=['Y'],\n",
    "                       false_values=['N'])\n",
    "\n",
    "    materials = pd.read_csv('../input/bill_of_materials.csv', index_col=0)\n",
    "    specs = pd.read_csv('../input/specs.csv', index_col=0)\n",
    "\n",
    "    for idx in tube.index:\n",
    "        if tube.ix[idx, 'material_id'] is not np.nan:\n",
    "            tube.ix[idx, tube.ix[idx, 'material_id']] = 1\n",
    "        for i in range(1, 11):\n",
    "            if i < 9 and materials.ix[idx, 'component_id_%d' % i] is not np.nan:\n",
    "                tube.ix[idx, materials.ix[idx, 'component_id_%d' % i]] = \\\n",
    "                    materials.ix[idx, 'quantity_%d' % i]\n",
    "            if specs.ix[idx, 'spec%d' % i] is not np.nan:\n",
    "                tube.ix[idx, specs.ix[idx, 'spec%d' % i]] = 1\n",
    "    tube.drop('material_id', inplace=True, axis=1)\n",
    "    tube.fillna(0, inplace=True)\n",
    "    </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. 0.2748 w/ RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "options(scipen = 10)\n",
    "\n",
    "###\n",
    "### Build train and test db\n",
    "###\n",
    "\n",
    "### Load train and test\n",
    "test = read.csv(\"../input/test_set.csv\")\n",
    "train = read.csv(\"../input/train_set.csv\")\n",
    "\n",
    "train$id = -(1:nrow(train))\n",
    "test$cost = 0\n",
    "\n",
    "train = rbind(train, test)\n",
    "\n",
    "### Merge datasets if only 1 variable in common\n",
    "continueLoop = TRUE\n",
    "while(continueLoop){\n",
    "  continueLoop = FALSE\n",
    "  for(f in dir(\"../input/\")){\n",
    "    d = read.csv(paste0(\"../input/\", f))\n",
    "    commonVariables = intersect(names(train), names(d))\n",
    "    if(length(commonVariables) == 1){\n",
    "      train = merge(train, d, by = commonVariables, all.x = TRUE)\n",
    "      continueLoop = TRUE\n",
    "      print(dim(train))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "### Clean NA values\n",
    "for(i in 1:ncol(train)){\n",
    "  if(is.numeric(train[,i])){\n",
    "    train[is.na(train[,i]),i] = -1\n",
    "  }else{\n",
    "    train[,i] = as.character(train[,i])\n",
    "    train[is.na(train[,i]),i] = \"NAvalue\"\n",
    "    train[,i] = as.factor(train[,i])\n",
    "  }\n",
    "}\n",
    "\n",
    "### Clean variables with too many categories\n",
    "for(i in 1:ncol(train)){\n",
    "  if(!is.numeric(train[,i])){\n",
    "    freq = data.frame(table(train[,i]))\n",
    "    freq = freq[order(freq$Freq, decreasing = TRUE),]\n",
    "    train[,i] = as.character(match(train[,i], freq$Var1[1:30]))\n",
    "    train[is.na(train[,i]),i] = \"rareValue\"\n",
    "    train[,i] = as.factor(train[,i])\n",
    "  }\n",
    "}\n",
    "\n",
    "test = train[which(train$id > 0),]\n",
    "train = train[which(train$id < 0),]\n",
    "\n",
    "###\n",
    "### Evaluate RF predictions by splitting the train db in 80%/20%\n",
    "###\n",
    "\n",
    "### Randomforest\n",
    "library(randomForest)\n",
    "\n",
    "# dtrain_cv = train[which(train$id %% 5 > 0),]\n",
    "# dtest_cv = train[which(train$id %% 5 == 0),]\n",
    "# \n",
    "# ### Train randomForest on dtrain_cv and evaluate predictions on dtest_cv\n",
    "# set.seed(123)\n",
    "# rf1 = randomForest(dtrain_cv$cost~., dtrain_cv[,-match(c(\"id\", \"cost\"), names(dtrain_cv))], ntree = 10, do.trace = 2)\n",
    "# \n",
    "# pred = predict(rf1, dtest_cv)\n",
    "# sqrt(mean((log(dtest_cv$cost + 1) - log(pred + 1))^2)) # 0.2589951\n",
    "# \n",
    "# ### With log transformation trick\n",
    "# set.seed(123)\n",
    "# rf2 = randomForest(log(dtrain_cv$cost + 1)~., dtrain_cv[,-match(c(\"id\", \"cost\"), names(dtrain_cv))], ntree = 10, do.trace = 2)\n",
    "# pred = exp(predict(rf2, dtest_cv)) - 1\n",
    "# \n",
    "# sqrt(mean((log(dtest_cv$cost + 1) - log(pred + 1))^2)) # 0.2410004\n",
    "\n",
    "### Train randomForest on the whole training set\n",
    "rf = randomForest(log(train$cost + 1)~., train[,-match(c(\"id\", \"cost\"), names(train))], ntree = 20, do.trace = 2)\n",
    "\n",
    "pred = exp(predict(rf, test)) - 1\n",
    "\n",
    "submitDb = data.frame(id = test$id, cost = pred)\n",
    "submitDb = aggregate(data.frame(cost = submitDb$cost), by = list(id = submitDb$id), mean)\n",
    "\n",
    "write.csv(submitDb, \"submit.csv\", row.names = FALSE, quote = FALSE)\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
