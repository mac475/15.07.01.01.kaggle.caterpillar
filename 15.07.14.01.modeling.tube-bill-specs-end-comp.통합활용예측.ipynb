{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "//    div.cell{\n",
       "//        width: 100%;\n",
       "//    }\n",
       "    // 아래의 div.container는 내가 임의로 추가한 style임\n",
       "//    div.container{\n",
       "//        width: 105%;\n",
       "//    }\n",
       "    ul {\n",
       "        line-height: 100%;\n",
       "        font-size: 100%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.5em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif, \"malgun gothic\";\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "//        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "//        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        font-family: malgun gothic;\n",
       "        line-height: 140%;\n",
       "//        font-size: 100%;\n",
       "//        width: 100%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-family: malgun gothic;\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "//                \"HTML-CSS\": {\n",
       "//                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "//                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0. Mode 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = True\n",
    "# mode = False\n",
    "\n",
    "predict_counter = 300\n",
    "\n",
    "# bracket_pricing별로 구분한다\n",
    "# bracket = 'Yes'\n",
    "# bracket = 'No'\n",
    "bracket = 'All'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0. Dataset 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv( './dataset/train_set.csv' )    # data를 읽어들인다.\n",
    "df_test = pd.read_csv( './dataset/test_set.csv' )    # data를 읽어들인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bracket == 'Yes' :\n",
    "    # bracket_pricing == Yes인 경우만\n",
    "    df_train = df_train[ df_train[ 'bracket_pricing' ] == 'Yes' ]\n",
    "elif bracket == 'No' :\n",
    "    # bracket_pricing == No인 경우만\n",
    "    df_train = df_train[ df_train[ 'bracket_pricing' ] == 'No' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Feature 추출 function\n",
    "* quote_date 활용하여 year, month, day 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def extract_year_month_day_from_quote_date( p_df ) :    # quote date로부터 연도, 월, 날짜를 추출\n",
    "    p_df[ 'quote_date' ] = pd.to_datetime( p_df[ 'quote_date' ] )    # string을 datetime으로 형변환\n",
    "    p_df[ 'year' ] = p_df[ 'quote_date' ].dt.year    # 연도\n",
    "    p_df[ 'month' ] = p_df[ 'quote_date' ].dt.month    # 월\n",
    "    p_df[ 'day' ] = p_df[ 'quote_date' ].dt.day    # 일\n",
    "    \n",
    "#     p_df[ 'weekofyear' ] = p_df[ 'quote_date' ].dt.weekofyear    # 주차\n",
    "#     p_df[ 'dayofyear' ] = p_df[ 'quote_date' ].dt.dayofyear    # 연중 일자\n",
    "#     p_df[ 'quarter' ] = p_df[ 'quote_date' ].dt.quarter    # 분기\n",
    "#     p_df[ 'weekday' ] = p_df[ 'quote_date' ].dt.weekday    # 요일\n",
    "    \n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_string_date( p_df ) :    # 일자정보를 string으로 변경\n",
    "    y = p_df[ 'year' ]\n",
    "    m = p_df[ 'month' ]\n",
    "    d = p_df[ 'day' ]\n",
    "    \n",
    "    str_y = ''\n",
    "    str_m = ''\n",
    "    str_d = ''\n",
    "    \n",
    "    str_y = str( y )\n",
    "    str_m = ''\n",
    "    if m < 10 :\n",
    "        str_m = '0' + str( m )\n",
    "    else :\n",
    "        str_m = str( m )\n",
    "    str_d = ''\n",
    "    if d < 10 :\n",
    "        str_d = '0' + str( d )\n",
    "    else :\n",
    "        str_d = str( d )\n",
    "    \n",
    "    return str_y + str_m + str_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Label Encoding function\n",
    "* 현재, train/ test dataset과 tube dataset과 merge하는 case까지 반영하여 encoder 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# le_bracket_pricing = preprocessing.LabelEncoder()\n",
    "le_supplier = preprocessing.LabelEncoder()\n",
    "le_material_id = preprocessing.LabelEncoder()\n",
    "le_date = preprocessing.LabelEncoder()\n",
    "le_year_month = preprocessing.LabelEncoder()\n",
    "\n",
    "le_yn = preprocessing.LabelEncoder()\n",
    "le_yn.fit( [ 'Y', 'N' ] )\n",
    "\n",
    "\n",
    "le_end = preprocessing.LabelEncoder()\n",
    "df_end = pd.read_csv( './dataset/01.original.dataset/tube_end_form.csv' )\n",
    "le_end.fit( df_end[ 'end_form_id' ] )\n",
    "df_end = ''\n",
    "\n",
    "\n",
    "le_component_id = preprocessing.LabelEncoder()    # component의 경우, master dataset이 별도로 존재하므로\n",
    "df_components = pd.read_csv( './dataset/02.ml.verified.dataset/components.verified.csv' )\n",
    "le_component_id.fit( df_components[ 'component_id' ] )\n",
    "\n",
    "\n",
    "le_spec_id = preprocessing.LabelEncoder()    # spec의 경우, spec meta dataset을 별도로 생성했음\n",
    "df_spec_meta = pd.read_csv( './dataset/01.original.dataset/spec_meta.csv' )\n",
    "le_spec_id.fit( df_spec_meta[ 'spec' ] )\n",
    "df_spec_meta = ''\n",
    "\n",
    "\n",
    "le_component_mac475 = preprocessing.LabelEncoder()\n",
    "le_component_mac475.fit( df_components[ 'component_mac475' ] )\n",
    "\n",
    "\n",
    "le_component_type_id = preprocessing.LabelEncoder()\n",
    "le_component_type_id.fit( df_components[ 'component_type_id' ] )\n",
    "df_components = ''\n",
    "\n",
    "\n",
    "le_type_end = preprocessing.LabelEncoder()\n",
    "df_type_end = pd.read_csv( './dataset/01.original.dataset/type_end_form.csv' )\n",
    "le_type_end.fit( df_type_end[ 'end_form_id' ] )\n",
    "df_type_end = ''\n",
    "\n",
    "\n",
    "le_connection = preprocessing.LabelEncoder()\n",
    "df_connection = pd.read_csv( './dataset/01.original.dataset/type_connection.csv' )\n",
    "le_connection.fit( df_connection[ 'connection_type_id' ] )\n",
    "df_connection = ''\n",
    "\n",
    "\n",
    "le_yn_none = preprocessing.LabelEncoder()\n",
    "le_yn_none.fit( [ 'Y', 'N', 'NONE' ] )\n",
    "\n",
    "\n",
    "le_boss_type = preprocessing.LabelEncoder()\n",
    "le_boss_type.fit( [ 'Boss', 'Stud', 'NONE' ] )\n",
    "\n",
    "\n",
    "le_elbow_mj_class_code = preprocessing.LabelEncoder()\n",
    "le_elbow_mj_class_code.fit( [ 'N', 'MJ-001', 'MJ-003', 'MJ-005', 'MJ-006', 'NONE' ] )\n",
    "\n",
    "le_boss_outside_shape = preprocessing.LabelEncoder()\n",
    "le_boss_outside_shape.fit( [ 'Round', 'Hex', 'NONE' ] )\n",
    "\n",
    "\n",
    "le_boss_base_type = preprocessing.LabelEncoder()\n",
    "le_boss_base_type.fit( [ 'Flat Bottom', 'Saddle', 'Shoulder', 'NONE' ] )\n",
    "\n",
    "def executeLabelEncoding( p_df, is_init ) :\n",
    "    if is_init == True :    # training dataset인 경우, label encoder 생성 및 fitting 수행\n",
    "        p_df[ 'supplier' ] = le_supplier.fit_transform( p_df[ 'supplier' ] )\n",
    "        p_df[ 'material_id' ] = le_material_id.fit_transform( p_df[ 'material_id' ] )\n",
    "        p_df[ 'date' ] = le_date.fit_transform( p_df[ 'date' ] )\n",
    "        p_df[ 'year_month' ] = le_year_month.fit_transform( p_df[ 'year_month' ] )\n",
    "    else :\n",
    "        p_df[ 'supplier' ] = le_supplier.transform( p_df[ 'supplier' ] )\n",
    "        p_df[ 'material_id' ] = le_material_id.transform( p_df[ 'material_id' ] )\n",
    "        p_df[ 'date' ] = le_date.transform( p_df[ 'date' ] )\n",
    "        p_df[ 'year_month' ] = le_year_month.transform( p_df[ 'year_month' ] )\n",
    "        \n",
    "    # 무조건 수행\n",
    "    p_df[ 'is_annual' ] = le_yn.transform( p_df[ 'is_annual' ] )\n",
    "    p_df[ 'bracket_pricing' ] = le_yn.transform( p_df[ 'bracket_pricing' ] )\n",
    "    \n",
    "    # tube/material_id == null을 tube만 사용예측시 적용, bill/ spec 같이 사용예측시 주석처리\n",
    "#     p_df[ 'end_a_1x' ] = le_yn.transform( p_df[ 'end_a_1x' ] )\n",
    "#     p_df[ 'end_a_2x' ] = le_yn.transform( p_df[ 'end_a_2x' ] )\n",
    "#     p_df[ 'end_x_1x' ] = le_yn.transform( p_df[ 'end_x_1x' ] )\n",
    "#     p_df[ 'end_x_2x' ] = le_yn.transform( p_df[ 'end_x_2x' ] )\n",
    "    \n",
    "    p_df[ 'end_a' ] = le_end.transform( p_df[ 'end_a' ] )\n",
    "    p_df[ 'end_x' ] = le_end.transform( p_df[ 'end_x' ] )\n",
    "\n",
    "    for i in range( 1, 9 ) :    # bill_of_materials에서 merge된 component_id_1~8을 label encoding 수행\n",
    "        comp_str = 'component_id_' + str( i )\n",
    "        p_df[ comp_str ] = le_component_id.transform( p_df[ comp_str ] )\n",
    "\n",
    "    for i in range( 1, 11 ) :    # specs에서 merge된 spec1~10을 label encoding 수행\n",
    "        spec_str = 'spec' + str( i )\n",
    "        p_df[ spec_str ] = le_spec_id.transform( p_df[ spec_str ] )                \n",
    "        \n",
    "    p_df[ 'forming_x' ] = le_yn.transform( p_df[ 'forming_x' ] )\n",
    "    p_df[ 'forming_y' ] = le_yn.transform( p_df[ 'forming_y' ] )\n",
    "    \n",
    "    \n",
    "    comp_list = [ 'x' ]\n",
    "    for v in comp_list :\n",
    "        v = ''\n",
    "        p_df[ 'component_type_id' + v ] = le_component_type_id.transform( p_df[ 'component_type_id' + v ] )\n",
    "        p_df[ 'component_mac475' + v ] = le_component_mac475.transform( p_df[ 'component_mac475' + v ] )\n",
    "\n",
    "\n",
    "        p_df[ 'adaptor_end_form_id_1' + v ] = le_type_end.transform( p_df[ 'adaptor_end_form_id_1' + v ] )\n",
    "        p_df[ 'adaptor_connection_type_id_1' + v ] = le_connection.transform( p_df[ 'adaptor_connection_type_id_1' + v ] )\n",
    "        p_df[ 'adaptor_end_form_id_2' + v ] = le_type_end.transform( p_df[ 'adaptor_end_form_id_2' + v ] )\n",
    "        p_df[ 'adaptor_connection_type_id_2' + v ] = le_connection.transform( p_df[ 'adaptor_connection_type_id_2' + v ] )\n",
    "        p_df[ 'adaptor_unique_feature' + v ] = le_yn_none.transform( p_df[ 'adaptor_unique_feature' + v ] )\n",
    "        p_df[ 'adaptor_orientation' + v ] = le_yn_none.transform( p_df[ 'adaptor_orientation' + v ] )\n",
    "\n",
    "\n",
    "        p_df[ 'boss_type' + v ] = le_boss_type.transform( p_df[ 'boss_type' + v ] )\n",
    "        p_df[ 'boss_connection_type_id' + v ] = le_connection.transform( p_df[ 'boss_connection_type_id' + v ] )\n",
    "        p_df[ 'boss_outside_shape' + v ] = le_boss_outside_shape.transform( p_df[ 'boss_outside_shape' + v ] )\n",
    "        p_df[ 'boss_base_type' + v ] = le_boss_base_type.transform( p_df[ 'boss_base_type' + v ] )\n",
    "        p_df[ 'boss_groove' + v ] = le_yn_none.transform( p_df[ 'boss_groove' + v ] )\n",
    "        p_df[ 'boss_unique_feature' + v ] = le_yn_none.transform( p_df[ 'boss_unique_feature' + v ] )\n",
    "        p_df[ 'boss_orientation' + v ] = le_yn_none.transform( p_df[ 'boss_orientation' + v ] )\n",
    "\n",
    "\n",
    "        p_df[ 'elbow_mj_class_code' + v ] = le_elbow_mj_class_code.transform( p_df[ 'elbow_mj_class_code' + v ] )\n",
    "        p_df[ 'elbow_mj_plug_class_code' + v ] = le_elbow_mj_class_code.transform( p_df[ 'elbow_mj_plug_class_code' + v ] )\n",
    "        p_df[ 'elbow_groove' + v ] = le_yn_none.transform( p_df[ 'elbow_groove' + v ] )\n",
    "        p_df[ 'elbow_unique_feature' + v ] = le_yn_none.transform( p_df[ 'elbow_unique_feature' + v ] )\n",
    "        p_df[ 'elbow_orientation' + v ] = le_yn_none.transform( p_df[ 'elbow_orientation' + v ] )\n",
    "\n",
    "        p_df[ 'float_orientation' + v ] = le_yn_none.transform( p_df[ 'float_orientation' + v ] )\n",
    "\n",
    "        p_df[ 'hfl_corresponding_shell' + v ] = le_yn_none.transform( p_df[ 'hfl_corresponding_shell' + v ] )\n",
    "        p_df[ 'hfl_coupling_class' + v ] = le_yn_none.transform( p_df[ 'hfl_coupling_class' + v ] )\n",
    "        p_df[ 'hfl_material' + v ] = le_yn_none.transform( p_df[ 'hfl_material' + v ] )\n",
    "        p_df[ 'hfl_plating' + v ] = le_yn_none.transform( p_df[ 'hfl_plating' + v ] )\n",
    "        p_df[ 'hfl_orientation' + v ] = le_yn_none.transform( p_df[ 'hfl_orientation' + v ] )\n",
    "\n",
    "        p_df[ 'nut_blind_hole' + v ] = le_yn_none.transform( p_df[ 'nut_blind_hole' + v ] )\n",
    "        p_df[ 'nut_orientation' + v ] = le_yn_none.transform( p_df[ 'nut_orientation' + v ] )\n",
    "\n",
    "        p_df[ 'sleeve_connection_type_id' + v ] = le_connection.transform( p_df[ 'sleeve_connection_type_id' + v ] )    \n",
    "        p_df[ 'sleeve_unique_feature' + v ] = le_yn_none.transform( p_df[ 'sleeve_unique_feature' + v ] )\n",
    "        p_df[ 'sleeve_plating' + v ] = le_yn_none.transform( p_df[ 'sleeve_plating' + v ] )\n",
    "        p_df[ 'sleeve_orientation' + v ] = le_yn_none.transform( p_df[ 'sleeve_orientation' + v ] )\n",
    "\n",
    "\n",
    "        p_df[ 'straight_mj_class_code' + v ] = le_elbow_mj_class_code.transform( p_df[ 'straight_mj_class_code' + v ] )\n",
    "        p_df[ 'straight_groove' + v ] = le_yn_none.transform( p_df[ 'straight_groove' + v ] )\n",
    "        p_df[ 'straight_unique_feature' + v ] = le_yn_none.transform( p_df[ 'straight_unique_feature' + v ] )\n",
    "        p_df[ 'straight_orientation' + v ] = le_yn_none.transform( p_df[ 'straight_orientation' + v ] )\n",
    "\n",
    "\n",
    "        p_df[ 'tee_mj_class_code' + v ] = le_elbow_mj_class_code.transform( p_df[ 'tee_mj_class_code' + v ] )\n",
    "        p_df[ 'tee_mj_plug_class_code' + v ] = le_elbow_mj_class_code.transform( p_df[ 'tee_mj_plug_class_code' + v ] )\n",
    "        p_df[ 'tee_groove' + v ] = le_yn_none.transform( p_df[ 'tee_groove' + v ] )    \n",
    "        p_df[ 'tee_unique_feature' + v ] = le_yn_none.transform( p_df[ 'tee_unique_feature' + v ] )    \n",
    "        p_df[ 'tee_orientation' + v ] = le_yn_none.transform( p_df[ 'tee_orientation' + v ] )    \n",
    "\n",
    "        p_df[ 'threaded_end_form_id_1' + v ] = le_type_end.transform( p_df[ 'threaded_end_form_id_1' + v ] )\n",
    "        p_df[ 'threaded_connection_type_id_1' + v ] = le_connection.transform( p_df[ 'threaded_connection_type_id_1' + v ] )    \n",
    "        p_df[ 'threaded_end_form_id_2' + v ] = le_type_end.transform( p_df[ 'threaded_end_form_id_2' + v ] )\n",
    "        p_df[ 'threaded_connection_type_id_2' + v ] = le_connection.transform( p_df[ 'threaded_connection_type_id_2' + v ] )\n",
    "        p_df[ 'threaded_end_form_id_3' + v ] = le_type_end.transform( p_df[ 'threaded_end_form_id_3' + v ] )\n",
    "        p_df[ 'threaded_connection_type_id_3' + v ] = le_connection.transform( p_df[ 'threaded_connection_type_id_3' + v ] )\n",
    "        p_df[ 'threaded_end_form_id_4' + v ] = le_end.transform( p_df[ 'threaded_end_form_id_4' + v ] )\n",
    "        p_df[ 'threaded_connection_type_id_4' + v ] = le_connection.transform( p_df[ 'threaded_connection_type_id_4' + v ] )\n",
    "        p_df[ 'threaded_unique_feature' + v ] = le_yn_none.transform( p_df[ 'threaded_unique_feature' + v ] )\n",
    "        p_df[ 'threaded_orientation' + v ] = le_yn_none.transform( p_df[ 'threaded_orientation' + v ] )\n",
    "\n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Label Encoding용 Dataset\n",
    "* categorical data를 모두 포함해야 하므로, tube join후 기반으로 label encoder를 fitting한다\n",
    "* label encoding의 대상은 bracket_pricing, supplier, material_id, end_a_1x 시리즈, end_a 시리즈 feature이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 train/ test dataset merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[ 'id' ] = 99999    # test와 join위해 feature 추가 : 99999는 train dataset이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged = df_train.append( df_test )    # train과 test df를 merge한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 tube_bill_specs dataset merge 및 label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tube_bill_specs_end_comp = pd.read_csv( './dataset/03.merged/tube_bill_specs_end_comp_merged.csv' )\n",
    "# df_tube_bill_specs_end_comp = pd.read_csv( './dataset/03.merged/tube_bill_specs_end_merged.csv' )\n",
    "\n",
    "# 임시코드\n",
    "df_tube_bill_specs_end_comp.drop( [ 'length_1','length_2','length_3','length_4',\n",
    "                                    'length_5','length_6','length_7','length_8',\n",
    "#                                     'length_sum'\n",
    "                                  ], axis = 1, inplace = True )\n",
    "# 임시코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = { 'Yes' : 'Y', 'No' : 'N' }\n",
    "df_merged[ 'bracket_pricing' ].replace( dic, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_is_annual( p_df ) :    # annual_usage 존재여부를 확인\n",
    "    is_annual = 'N'\n",
    "    if p_df[ 'annual_usage' ] > 0 :\n",
    "        is_annual = 'Y'\n",
    "    return is_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged[ 'is_annual' ] = df_merged.apply( get_is_annual, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min_order_quantity 조건테스트\n",
    "df_merged[ 'min_order_quantity' ].replace( to_replace = 0, value = 1, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_order_count( p_df, p_type ) :    # 주문횟수를 계산\n",
    "    order_count = 0\n",
    "    \n",
    "    if p_type == 'min' :\n",
    "        if ( p_df[ 'annual_usage' ] > 0 ) & ( p_df[ 'min_order_quantity' ] > 0 ) :\n",
    "            order_count = float( p_df[ 'annual_usage' ] / p_df[ 'min_order_quantity' ] )\n",
    "    elif p_type == 'quantity' :\n",
    "        order_count = float( p_df[ 'annual_usage' ] / p_df[ 'quantity' ] )\n",
    "    elif p_type == 'rule' :\n",
    "        if p_df[ 'bracket_pricing' ] == 'Y' :\n",
    "            order_count = float( p_df[ 'annual_usage' ] / p_df[ 'quantity' ] )\n",
    "        elif p_df[ 'bracket_pricing' ] == 'N' :\n",
    "            order_count = float( p_df[ 'annual_usage' ] / p_df[ 'min_order_quantity' ] )\n",
    "    elif p_type == 'sales' :\n",
    "        order_count = float( p_df[ 'annual_usage' ] * p_df[ 'quantity' ] )\n",
    "            \n",
    "    return order_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 883 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_merged[ 'sales_result' ] = df_merged.apply( get_order_count, axis = 1, args = ('sales',) )\n",
    "df_merged = extract_year_month_day_from_quote_date( df_merged )    # feature 처리를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_merged[ 'date' ] = df_merged.apply( make_string_date, axis = 1 )\n",
    "df_merged[ 'year_month' ] = df_merged[ 'date' ].str[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = df_merged.merge( df_tube_bill_specs_end_comp, how = 'inner', on = 'tube_assembly_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_merged = executeLabelEncoding( df_merged, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight_sum, length_sum의 nomalization을 위한 train/ test merged df에서의 weight_sum_max, length_sum_max\n",
    "weight_sum_max = df_merged[ 'weight_sum' ].max()\n",
    "length_sum_max = df_merged[ 'length_sum' ].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_merged.to_csv( './dataset/df_train_test_tube_bill_specs_end_comp_merged_encoded.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.drop( 'id', axis = 1, inplace = True )    # 일단, 필요없는 것들 제거 : df_train을 원상 복구해둠\n",
    "del( df_test )    # 일단, 필요없는 것들 제거\n",
    "del( df_merged )    # 일단, 필요없는 것들 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 Merged features중, modeling시 제거할 feature 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_for_remove = []    # 전역변수의 선언\n",
    "def executeFeatureRemoval( p_df ) :\n",
    "    # 제거하고자 하는 feature list\n",
    "    global list_for_remove    # 전역변수 명시\n",
    "    \n",
    "    list_for_remove = [\n",
    "                        'tube_assembly_id',\n",
    "                        'quote_date',\n",
    "#                         'date',\n",
    "\n",
    "#                         'supplier',        \n",
    "#                         'annual_usage',\n",
    "#                         'min_order_quantity',\n",
    "#                         'bracket_pricing',\n",
    "        \n",
    "#                         'order_count_by_quantity',    # Case 88에서 제거시도\n",
    "#                         'order_count_by_min',    # Case 88에서 제거시도\n",
    "#                         'order_count_by_rule',    # Case 88에서 제거시도\n",
    "#                         'sales_result',\n",
    "        \n",
    "        \n",
    "#                         'quantity',\n",
    "#                         'material_id',\n",
    "#                         'diameter',\n",
    "#                         'wall',\n",
    "#                         'length',\n",
    "#                         'num_bends',\n",
    "#                         'bend_radius',\n",
    "#                         'end_a_1x',\n",
    "#                         'end_a_2x',\n",
    "#                         'end_x_1x',\n",
    "#                         'end_x_2x',\n",
    "#                         'end_a',\n",
    "#                         'end_x',\n",
    "#                         'num_boss',\n",
    "#                         'num_bracket',\n",
    "#                         'other',\n",
    "        \n",
    "#                         'component_id_1',\n",
    "#                         'quantity_1',\n",
    "#                         'component_id_2',\n",
    "#                         'quantity_2',\n",
    "#                         'component_id_3',\n",
    "#                         'quantity_3',\n",
    "#                         'component_id_4',\n",
    "#                         'quantity_4',\n",
    "\n",
    "#                         'component_id_5',\n",
    "#                         'quantity_5',\n",
    "#                         'component_id_6',\n",
    "#                         'quantity_6',\n",
    "#                         'component_id_7',\n",
    "#                         'quantity_7',\n",
    "#                         'component_id_8',\n",
    "#                         'quantity_8',\n",
    "        \n",
    "#                         'weight_sum',\n",
    "#                         'adaptor_sum',\n",
    "#                         'boss_sum',\n",
    "#                         'elbow_sum',\n",
    "#                         'float_sum',\n",
    "#                         'hfl_sum',\n",
    "#                         'nut_sum',\n",
    "#                         'other_sum',\n",
    "#                         'sleeve_sum',\n",
    "#                         'straight_sum',\n",
    "#                         'tee_sum',\n",
    "#                         'threaded_sum',\n",
    "        \n",
    "#                         'adaptor_weight_sum',\n",
    "#                         'boss_weight_sum',\n",
    "#                         'elbow_weight_sum',\n",
    "#                         'float_weight_sum',\n",
    "#                         'hfl_weight_sum',\n",
    "#                         'nut_weight_sum',\n",
    "#                         'other_weight_sum',\n",
    "#                         'sleeve_weight_sum',\n",
    "#                         'straight_weight_sum',\n",
    "#                         'tee_weight_sum',\n",
    "#                         'threaded_weight_sum',\n",
    "        \n",
    "#                         'comp_type_count',\n",
    "#                         'comp_total_count',\n",
    "#                         'tube_volume',\n",
    "        \n",
    "#                         'spec1',\n",
    "#                         'spec2',\n",
    "#                         'spec3',\n",
    "#                         'spec4',\n",
    "#                         'spec5',\n",
    "        \n",
    "#                         'spec6',\n",
    "#                         'spec7',\n",
    "#                         'spec8',\n",
    "#                         'spec9',\n",
    "#                         'spec10',\n",
    "        \n",
    "#                         'spec_type_count',\n",
    "#                         'forming_x',\n",
    "#                         'forming_y',\n",
    "\n",
    "                        # component family 계열의 merge를 하지않은 case에 대한 feature들을 일단 죽여둔다\n",
    "                        'component_type_id',\n",
    "                        'mac475_weight',\n",
    "        \n",
    "#                         'component_mac475',    # Case 87\n",
    "                        \n",
    "                        'uniqueness',\n",
    "                        'orientation',\n",
    "        \n",
    "                        'adaptor_adaptor_angle', 'adaptor_overall_length', 'adaptor_end_form_id_1' ,\n",
    "                        'adaptor_connection_type_id_1', 'adaptor_length_1', 'adaptor_thread_size_1',\n",
    "                        'adaptor_thread_pitch_1','adaptor_nominal_size_1', 'adaptor_end_form_id_2',\n",
    "                        'adaptor_connection_type_id_2','adaptor_length_2', 'adaptor_thread_size_2',\n",
    "                        'adaptor_thread_pitch_2', 'adaptor_nominal_size_2', 'adaptor_hex_size',\n",
    "                        'adaptor_unique_feature', 'adaptor_orientation', 'adaptor_weight',\n",
    "\n",
    "                        'boss_type', 'boss_connection_type_id', 'boss_outside_shape', 'boss_base_type',\n",
    "                        'boss_height_over_tube', 'boss_bolt_pattern_long', 'boss_bolt_pattern_wide',\n",
    "                        'boss_groove', 'boss_base_diameter', 'boss_shoulder_diameter', 'boss_unique_feature',\n",
    "                        'boss_orientation', 'boss_weight',\n",
    "\n",
    "                        'elbow_bolt_pattern_long', 'elbow_bolt_pattern_wide', 'elbow_extension_length',\n",
    "                        'elbow_overall_length', 'elbow_thickness', 'elbow_drop_length', 'elbow_elbow_angle',\n",
    "                        'elbow_mj_class_code', 'elbow_mj_plug_class_code', 'elbow_plug_diameter',\n",
    "                        'elbow_groove', 'elbow_unique_feature', 'elbow_orientation', 'elbow_weight',\n",
    "\n",
    "                        'float_bolt_pattern_long', 'float_bolt_pattern_wide', 'float_thickness',\n",
    "                        'float_orientation', 'float_weight',\n",
    "\n",
    "                        'hfl_hose_diameter', 'hfl_corresponding_shell', 'hfl_coupling_class', 'hfl_material',\n",
    "                        'hfl_plating', 'hfl_orientation', 'hfl_weight', \n",
    "        \n",
    "                        'nut_hex_nut_size', 'nut_seat_angle', 'nut_length', 'nut_thread_size', 'nut_thread_pitch',\n",
    "                        'nut_diameter', 'nut_blind_hole', 'nut_orientation', 'nut_weight',\n",
    "\n",
    "                        'other_weight',\n",
    "\n",
    "                        'sleeve_connection_type_id', 'sleeve_length', 'sleeve_intended_nut_thread',\n",
    "                        'sleeve_intended_nut_pitch', 'sleeve_unique_feature', 'sleeve_plating', 'sleeve_orientation',\n",
    "                        'sleeve_weight',\n",
    "\n",
    "                        'straight_bolt_pattern_long', 'straight_bolt_pattern_wide', 'straight_head_diameter',\n",
    "                        'straight_overall_length', 'straight_thickness', 'straight_mj_class_code',\n",
    "                        'straight_groove', 'straight_unique_feature', 'straight_orientation', 'straight_weight',\n",
    "\n",
    "                        'tee_bolt_pattern_long', 'tee_bolt_pattern_wide', 'tee_extension_length',\n",
    "                        'tee_overall_length', 'tee_thickness', 'tee_drop_length', 'tee_mj_class_code',\n",
    "                        'tee_mj_plug_class_code', 'tee_groove', 'tee_unique_feature', 'tee_orientation', 'tee_weight',\n",
    "\n",
    "                        'threaded_adaptor_angle', 'threaded_overall_length', 'threaded_hex_size', 'threaded_end_form_id_1',\n",
    "                        'threaded_connection_type_id_1', 'threaded_length_1', 'threaded_thread_size_1',\n",
    "                        'threaded_thread_pitch_1', 'threaded_nominal_size_1', 'threaded_end_form_id_2',\n",
    "                        'threaded_connection_type_id_2', 'threaded_length_2', 'threaded_thread_size_2',\n",
    "                        'threaded_thread_pitch_2', 'threaded_nominal_size_2', 'threaded_end_form_id_3',\n",
    "                        'threaded_connection_type_id_3', 'threaded_length_3', 'threaded_thread_size_3',\n",
    "                        'threaded_thread_pitch_3', 'threaded_nominal_size_3', 'threaded_end_form_id_4',\n",
    "                        'threaded_connection_type_id_4', 'threaded_length_4', 'threaded_thread_size_4',\n",
    "                        'threaded_thread_pitch_4', 'threaded_nominal_size_4', 'threaded_unique_feature',\n",
    "                        'threaded_orientation', 'threaded_weight',\n",
    "\n",
    "                        # 예측결과 0.0000 영향도인 features\n",
    "        \n",
    "                        # 유의미 추정가능한 부활가능 Feature들 시작        \n",
    "                        'num_bracket',\n",
    "        \n",
    "                        'component_id_4', 'component_id_5', 'component_id_6',        \n",
    "                        'spec4','spec6','spec7','spec8',\n",
    "                        'quantity_2','quantity_3','quantity_4','quantity_5','quantity_6','quantity_7',\n",
    "                        'sleeve_weight_sum','hfl_weight_sum','float_weight_sum','boss_weight_sum','adaptor_weight_sum',\n",
    "                        'tee_sum','straight_sum','sleeve_sum','hfl_sum','boss_sum','adaptor_sum','elbow_sum',\n",
    "                        'comp_total_count',\n",
    "        \n",
    "#                         'bracket_pricing',        \n",
    "                        'forming_y','forming_x',\n",
    "                        'spec_type_count','comp_type_count',\n",
    "                        # 유의미 추정가능한 부활가능 Feature들 종료\n",
    "\n",
    "                        'is_annual',\n",
    "                        'component_id_7','component_id_8',\n",
    "                        'spec9','spec10',\n",
    "                        'uniqueness_count',\n",
    "                        'quantity_8',\n",
    "                        'tee_weight_sum',\n",
    "                        'SP-0060', 'SP-0063', 'SP-0062', 'SP-0064', 'SP-0070', 'SP-0065', 'SP-0066', 'SP-0067', 'SP-0068',\n",
    "                        'SP-0051', 'SP-0059', 'SP-0056', 'SP-0055','SP-0054','SP-0053','SP-0052','SP-0072','SP-0050',\n",
    "                        'SP-0049','SP-0048',  'SP-0046','SP-0045', 'SP-0071','SP-0081','SP-0073','SP-0074',\n",
    "                        'SP-0096','SP-0095','SP-0094','SP-0093','SP-0092','SP-0091','SP-0090','SP-0089','SP-0088',\n",
    "                        'SP-0087','SP-0086','SP-0085','SP-0084','SP-0083','SP-0082','SP-0043','SP-0080','SP-0079','SP-0078',\n",
    "                        'SP-0077','SP-0076','SP-0075','SP-0044','SP-0038','SP-0042','SP-0001','SP-0002','SP-0003',\n",
    "                        'SP-0004','SP-0006','SP-0041','SP-0033','SP-0027','SP-0028','SP-0029','SP-0030','SP-0031',\n",
    "                        'SP-0032','SP-0034','SP-0007','SP-0035','SP-0036','SP-0037','SP-0039','SP-0040','SP-0026',\n",
    "                        'SP-0025','SP-0024','SP-0023','SP-0021','SP-0020','SP-0019','SP-0018','SP-0015','SP-0014',\n",
    "                        'SP-0013','SP-0012','SP-0011','SP-0010','SP-0009','SP-0008','SP-0017',\n",
    "                        'price_band_count',\n",
    "                        'bracket_yes_count', 'bracket_no_count',\n",
    "                        'tube_area',\n",
    "                        'overall_length',\n",
    "#                         'complexity',\n",
    "#                         'length_sum',\n",
    "                        'year_month',\n",
    "                      ]\n",
    "\n",
    "    return p_df.drop( list_for_remove, axis = 1, inplace = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 Train : 연/ 월/ 일 Feature 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = extract_year_month_day_from_quote_date( df_train )    # feature 처리를 수행\n",
    "df_train[ 'date' ] = df_train.apply( make_string_date, axis = 1 )\n",
    "df_train[ 'year_month' ] = df_train[ 'date' ].str[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.3 Train : Tube와 merge통한 Feature 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_merged = df_train.merge( df_tube_bill_specs_end_comp, how = 'inner', on = 'tube_assembly_id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train : Tube별 min, max quantity Feature 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tube_min_max_quantity = pd.read_csv( './dataset/01.original.dataset/tube.min.max.quantity.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_merged = df_train_merged.merge( df_tube_min_max_quantity, how = 'inner', on = 'tube_assembly_id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train : supplier annual 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_supplier_annual = pd.read_csv( './dataset/01.original.dataset/supplier_annual.csv' )\n",
    "# df_supplier_total = pd.read_csv( './dataset/01.original.dataset/supplier_total.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train_merged = df_train_merged.merge( df_supplier_annual, how = 'inner', on = [ 'supplier', 'year' ] )\n",
    "# df_train_merged = df_train_merged.merge( df_supplier_total, how = 'inner', on = [ 'supplier' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 Train : Merged dataset Label Encoding 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_merged[ 'bracket_pricing' ].replace( dic, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train_merged[ 'is_annual' ] = df_train_merged.apply( get_is_annual, axis = 1 )\n",
    "df_train_merged[ 'order_count_by_min' ] = df_train_merged.apply( get_order_count, axis = 1, args = ('min',) )\n",
    "df_train_merged[ 'order_count_by_quantity' ] = df_train_merged.apply( get_order_count, axis = 1, args = ('quantity',) )\n",
    "df_train_merged[ 'order_count_by_rule' ] = df_train_merged.apply( get_order_count, axis = 1, args = ('rule',) )\n",
    "df_train_merged[ 'sales_result' ] = df_train_merged.apply( get_order_count, axis = 1, args = ('sales',) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_merged[ 'req_comp_spec_type_count' ] = df_train_merged[ 'spec_type_count' ] + df_train_merged[ 'comp_type_count' ]\n",
    "df_train_merged[ 'req_comp_spec_total_count' ] = df_train_merged[ 'spec_type_count' ] + df_train_merged[ 'comp_type_count' ] * df_train_merged[ 'comp_total_count' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_end_ax_count( p_df ) :    # end_a, end_x의 count를 계산\n",
    "    count = 0\n",
    "    if p_df[ 'end_a' ] != 'NONE' :\n",
    "        count += 1\n",
    "    if p_df[ 'end_x' ] != 'NONE' :\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def check_is_end_ax_same( p_df ) :    # end_a, end_x가 동일한지 여부를 판단\n",
    "    is_same = 0\n",
    "    if p_df[ 'end_a' ] == p_df[ 'end_x' ] :\n",
    "        is_same = 1\n",
    "    return is_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train_merged[ 'end_ax_count' ] = df_train_merged.apply( calculate_end_ax_count, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train_merged[ 'is_end_ax_same' ] = df_train_merged.apply( check_is_end_ax_same, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5 Train : 불필요 Feature 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_train_merged.to_csv( './dataset/df_train_tube_bill_specs_end_comp_merged.csv' )\n",
    "# df_train_merged = executeLabelEncoding( df_train_merged, is_init = False )    # label encoding을 수행한다\n",
    "# df_train_merged.to_csv( './dataset/df_train_tube_bill_specs_end_comp_merged_encoded.csv' )\n",
    "# df_train_merged = executeFeatureRemoval( df_train_merged )    # feature removal 처리를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.6 Test : Dataset 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv( './dataset/test_set.csv' )    # data를 읽어들인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bracket == 'Yes' :\n",
    "    # bracket_pricing == Yes인 경우만\n",
    "    df_test = df_test[ df_test[ 'bracket_pricing' ] == 'Yes' ]\n",
    "elif bracket == 'No' :\n",
    "    # bracket_pricing == No인 경우만\n",
    "    df_test = df_test[ df_test[ 'bracket_pricing' ] == 'No' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.7 Test : 연/ 월/ 일 Feature 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test = extract_year_month_day_from_quote_date( df_test )    # feature 처리를 수행\n",
    "df_test[ 'date' ] = df_test.apply( make_string_date, axis = 1 )\n",
    "df_test[ 'year_month' ] = df_test[ 'date' ].str[0:6]\n",
    "\n",
    "df_result = pd.DataFrame( df_test[ 'id' ], columns = ['id'] )    # 결과용 dataframe을 생성\n",
    "df_test.drop( 'id', axis = 1, inplace = True )    # id feature는 일단 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.8 Test : Tube와 merge통한 Feature 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_merged = df_test.merge( df_tube_bill_specs_end_comp, how = 'inner', on = 'tube_assembly_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_merged = df_test_merged.merge( df_tube_min_max_quantity, how = 'inner', on = 'tube_assembly_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test_merged = df_test_merged.merge( df_supplier_annual, how = 'inner', on = [ 'supplier', 'year' ] )\n",
    "# df_test_merged = df_test_merged.merge( df_supplier_total, how = 'inner', on = [ 'supplier' ] )\n",
    "# del( df_supplier_annual )\n",
    "# del( df_supplier_total )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.9 Test : Merged dataset Label Encoding 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test_merged[ 'bracket_pricing' ].replace( dic, inplace = True )\n",
    "df_test_merged[ 'is_annual' ] = df_test_merged.apply( get_is_annual, axis = 1 )\n",
    "df_test_merged[ 'order_count_by_min' ] = df_test_merged.apply( get_order_count, axis = 1, args = ('min',) )\n",
    "df_test_merged[ 'order_count_by_quantity' ] = df_test_merged.apply( get_order_count, axis = 1, args = ('quantity',) )\n",
    "df_test_merged[ 'order_count_by_rule' ] = df_test_merged.apply( get_order_count, axis = 1, args = ('rule',) )\n",
    "df_test_merged[ 'sales_result' ] = df_test_merged.apply( get_order_count, axis = 1, args = ('sales',) )\n",
    "\n",
    "df_test_merged[ 'req_comp_spec_type_count' ] = df_test_merged[ 'spec_type_count' ] + df_test_merged[ 'comp_type_count' ]\n",
    "df_test_merged[ 'req_comp_spec_total_count' ] = df_test_merged[ 'spec_type_count' ] + df_test_merged[ 'comp_type_count' ] * df_test_merged[ 'comp_total_count' ]\n",
    "\n",
    "# df_test_merged[ 'end_ax_count' ] = df_test_merged.apply( calculate_end_ax_count, axis = 1 )\n",
    "# df_test_merged[ 'is_end_ax_same' ] = df_test_merged.apply( check_is_end_ax_same, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.10 Test : 불필요 Featrue 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bend num으로 인한 tube의 복잡도 추정 feature의 추가\n",
    "replaced_val = 1\n",
    "df_train_merged[ 'bend_num_by_radius' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'bend_radius' ]\n",
    "df_test_merged[ 'bend_num_by_radius' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'bend_radius' ]\n",
    "\n",
    "df_train_merged[ 'bend_num_X_volume' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'tube_volume' ]\n",
    "df_test_merged[ 'bend_num_X_volume' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'tube_volume' ]\n",
    "\n",
    "df_train_merged[ 'bend_num_X_length' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'length' ]\n",
    "df_test_merged[ 'bend_num_X_length' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'length' ]\n",
    "\n",
    "df_train_merged[ 'bend_num_X_diameter' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'diameter' ]\n",
    "df_test_merged[ 'bend_num_X_diameter' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'diameter' ]\n",
    "\n",
    "df_train_merged[ 'bend_num_X_wall' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'wall' ]\n",
    "df_test_merged[ 'bend_num_X_wall' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'wall' ]\n",
    "\n",
    "\n",
    "# df_train_merged[ 'bend_num_X_area' ] = df_train_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_train_merged[ 'tube_area' ]\n",
    "# df_test_merged[ 'bend_num_X_area' ] = df_test_merged[ 'num_bends' ].replace( 0, replaced_val ) * df_test_merged[ 'tube_area' ]\n",
    "\n",
    "df_train_merged[ 'quantity_by_max' ] = df_train_merged[ 'quantity' ] /  df_train_merged[ 'quantity_range_max' ]\n",
    "df_test_merged[ 'quantity_by_max' ] = df_test_merged[ 'quantity' ] /  df_test_merged[ 'quantity_range_max' ]\n",
    "df_train_merged[ 'quantity_by_min' ] = df_train_merged[ 'quantity' ] /  df_train_merged[ 'quantity_range_min' ]\n",
    "df_test_merged[ 'quantity_by_min' ] = df_test_merged[ 'quantity' ] /  df_test_merged[ 'quantity_range_min' ]\n",
    "\n",
    "# 추가\n",
    "df_train_merged[ 'bracket_num_X_volume' ] = df_train_merged[ 'num_bracket' ] * df_train_merged[ 'tube_volume' ]\n",
    "df_test_merged[ 'bracket_num_X_volume' ] = df_test_merged[ 'num_bracket' ] * df_test_merged[ 'tube_volume' ]\n",
    "df_train_merged[ 'bracket_num_X_wall' ] = df_train_merged[ 'num_bracket' ] * df_train_merged[ 'wall' ]\n",
    "df_test_merged[ 'bracket_num_X_wall' ] = df_test_merged[ 'num_bracket' ] * df_test_merged[ 'wall' ]\n",
    "\n",
    "# 복잡도들의 합\n",
    "df_train_merged[ 'complexity' ] = df_train_merged[ 'num_bracket' ] +\\\n",
    "                                  df_train_merged[ 'num_boss' ] + df_train_merged[ 'other' ]\n",
    "df_test_merged[ 'complexity' ] = df_test_merged[ 'num_bracket' ] +\\\n",
    "                                 df_test_merged[ 'num_boss' ] + df_test_merged[ 'other' ]\n",
    "# df_train_merged[ 'complexity' ] = df_train_merged[ 'num_bends' ] + df_train_merged[ 'num_bracket' ] +\\\n",
    "#                                   df_train_merged[ 'num_boss' ] + df_train_merged[ 'other' ]\n",
    "# df_test_merged[ 'complexity' ] = df_test_merged[ 'num_bends' ] + df_test_merged[ 'num_bracket' ] +\\\n",
    "#                                  df_test_merged[ 'num_boss' ] + df_test_merged[ 'other' ]\n",
    "df_train_merged[ 'complexity_X_volume' ] = df_train_merged[ 'complexity' ] * df_train_merged[ 'tube_volume' ]\n",
    "df_test_merged[ 'complexity_X_volume' ] = df_test_merged[ 'complexity' ] * df_test_merged[ 'tube_volume' ]\n",
    "df_train_merged[ 'complexity_X_diameter' ] = df_train_merged[ 'complexity' ] * df_train_merged[ 'diameter' ]\n",
    "df_test_merged[ 'complexity_X_diameter' ] = df_test_merged[ 'complexity' ] * df_test_merged[ 'diameter' ]\n",
    "\n",
    "\n",
    "# weight_sum, length_sum의 활용\n",
    "# df_train_merged[ 'weight_sum' ] = df_train_merged[ 'weight_sum' ] / weight_sum_max\n",
    "# df_test_merged[ 'weight_sum' ] = df_test_merged[ 'weight_sum' ] / weight_sum_max\n",
    "# df_train_merged[ 'length_sum' ] = df_train_merged[ 'length_sum' ] / length_sum_max\n",
    "# df_test_merged[ 'length_sum' ] = df_test_merged[ 'length_sum' ] / length_sum_max\n",
    "# df_train_merged[ 'weight_sum_plus_length_sum' ] = df_train_merged[ 'weight_sum' ] + df_train_merged[ 'length_sum' ]\n",
    "# df_test_merged[ 'weight_sum_plus_length_sum' ] = df_test_merged[ 'weight_sum' ] + df_test_merged[ 'length_sum' ] \n",
    "\n",
    "# #### weight_sum, length_sum, complexity\n",
    "# df_train_merged[ 'weight_sum_X_volume' ] = df_train_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'tube_volume' ]\n",
    "# df_test_merged[ 'weight_sum_X_volume' ] = df_test_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'tube_volume' ]\n",
    "\n",
    "# df_train_merged[ 'weight_sum_X_length' ] = df_train_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'length' ]\n",
    "# df_test_merged[ 'weight_sum_X_length' ] = df_test_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'length' ]\n",
    "\n",
    "# df_train_merged[ 'weight_sum_X_diameter' ] = df_train_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'diameter' ]\n",
    "# df_test_merged[ 'weight_sum_X_diameter' ] = df_test_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'diameter' ]\n",
    "\n",
    "# df_train_merged[ 'weight_sum_X_wall' ] = df_train_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'wall' ]\n",
    "# df_test_merged[ 'weight_sum_X_wall' ] = df_test_merged[ 'weight_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'wall' ]\n",
    "\n",
    "\n",
    "# df_train_merged[ 'length_sum_X_volume' ] = df_train_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'tube_volume' ]\n",
    "# df_test_merged[ 'length_sum_X_volume' ] = df_test_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'tube_volume' ]\n",
    "\n",
    "# df_train_merged[ 'length_sum_X_length' ] = df_train_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'length' ]\n",
    "# df_test_merged[ 'length_sum_X_length' ] = df_test_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'length' ]\n",
    "\n",
    "# df_train_merged[ 'length_sum_X_diameter' ] = df_train_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'diameter' ]\n",
    "# df_test_merged[ 'length_sum_X_diameter' ] = df_test_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'diameter' ]\n",
    "\n",
    "# df_train_merged[ 'length_sum_X_wall' ] = df_train_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_train_merged[ 'wall' ]\n",
    "# df_test_merged[ 'length_sum_X_wall' ] = df_test_merged[ 'length_sum' ].replace( 0, replaced_val ) * df_test_merged[ 'wall' ]\n",
    "\n",
    "\n",
    "# df_train_merged[ 'complexity_X_volume' ] = df_train_merged[ 'complexity' ].replace( 0, replaced_val ) * df_train_merged[ 'tube_volume' ]\n",
    "# df_test_merged[ 'complexity_X_volume' ] = df_test_merged[ 'complexity' ].replace( 0, replaced_val ) * df_test_merged[ 'tube_volume' ]\n",
    "\n",
    "# df_train_merged[ 'complexity_X_length' ] = df_train_merged[ 'complexity' ].replace( 0, replaced_val ) * df_train_merged[ 'length' ]\n",
    "# df_test_merged[ 'complexity_X_length' ] = df_test_merged[ 'complexity' ].replace( 0, replaced_val ) * df_test_merged[ 'length' ]\n",
    "\n",
    "# df_train_merged[ 'complexity_X_diameter' ] = df_train_merged[ 'complexity' ].replace( 0, replaced_val ) * df_train_merged[ 'diameter' ]\n",
    "# df_test_merged[ 'complexity_X_diameter' ] = df_test_merged[ 'complexity' ].replace( 0, replaced_val ) * df_test_merged[ 'diameter' ]\n",
    "\n",
    "# df_train_merged[ 'complexity_X_wall' ] = df_train_merged[ 'complexity' ].replace( 0, replaced_val ) * df_train_merged[ 'wall' ]\n",
    "# df_test_merged[ 'complexity_X_wall' ] = df_test_merged[ 'complexity' ].replace( 0, replaced_val ) * df_test_merged[ 'wall' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_to_log = [ 'tube_volume',\n",
    "                'bend_num_X_volume', 'bend_num_X_length', 'bend_num_X_diameter', 'bend_num_X_wall', 'bend_num_by_radius',\n",
    "                'bracket_num_X_volume',\n",
    "                'complexity_X_volume',\n",
    "                'complexity_X_diameter',\n",
    "              ]\n",
    "\n",
    "for feat in list_to_log :\n",
    "    df_train_merged[ feat ].replace( to_replace = 0, value = 1, inplace = True )\n",
    "    df_test_merged[ feat ].replace( to_replace = 0, value = 1, inplace = True )\n",
    "    \n",
    "    df_train_merged[ feat ] = np.log10( df_train_merged[ feat ] )\n",
    "    df_test_merged[ feat ] = np.log10( df_test_merged[ feat ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bracket != 'All' :\n",
    "    df_train_merged.drop( 'bracket_pricing', axis = 1, inplace = False )\n",
    "    df_test_merged.drop( 'bracket_pricing', axis = 1, inplace = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train_merged.to_csv( './dataset/df_train_tube_bill_specs_end_comp_merged.csv' )\n",
    "df_train_merged = executeLabelEncoding( df_train_merged, is_init = False )    # label encoding을 수행한다\n",
    "df_train_merged.to_csv( './dataset/df_train_tube_bill_specs_end_comp_merged_encoded.csv' )\n",
    "df_train_merged = executeFeatureRemoval( df_train_merged )    # feature removal 처리를 수행\n",
    "df_train_merged.to_csv( './dataset/df_train_tube_bill_specs_end_comp_merged_encoded_final.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test_merged = executeLabelEncoding( df_test_merged, is_init = False )    # label encoding을 수행한다\n",
    "df_test_merged = executeFeatureRemoval( df_test_merged )    # feature removal 처리를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.11 Prediction Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_train_merged.drop( 'cost', axis = 1, inplace = False )    # X를 확보\n",
    "y = np.log1p( df_train_merged[ 'cost' ] )   # y를 확보\n",
    "\n",
    "del( df_train )\n",
    "del( df_train_merged )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_cnt = 10    # cv : cross validation 횟수\n",
    "n_jobs_cnt = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if mode == True :\n",
    "    model_list = [\n",
    "                   GradientBoostingRegressor(\n",
    "                                              n_estimators = 2000,\n",
    "                                              max_depth = 7,\n",
    "                                              warm_start = True,\n",
    "                                              subsample = 0.8\n",
    "                                            ),\n",
    "                 ]\n",
    "else :\n",
    "    model_list = [\n",
    "#                    RandomForestRegressor(),    # 테스트용\n",
    "                   GradientBoostingRegressor(),    # 테스트용\n",
    "                 ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "for model in model_list :\n",
    "    for rep_counter in range( 0, predict_counter ) :    # rep_counter만큼 반복한다\n",
    "        model_name = str( model ).split( '(' )[0]    # 파일생성용\n",
    "\n",
    "        if model_name.startswith( 'GradientBoostingRegressor' ) :\n",
    "            if mode == True : \n",
    "                params = {\n",
    "    #                        'n_estimators' : (2500,3000),\n",
    "    #                        'max_depth' : (5,7,9),\n",
    "    #                        'subsample' : (1.0,0.9,0.8,0.7),\n",
    "    #                        'loss' : ('ls','lad','huber'),\n",
    "                         }\n",
    "            else :\n",
    "                params = {}\n",
    "        elif model_name.startswith( 'RandomForestRegressor' ) :\n",
    "            params = {\n",
    "                     }        \n",
    "\n",
    "        gs = grid_search.GridSearchCV( model,\n",
    "                                       param_grid = params,\n",
    "                                       n_jobs = n_jobs_cnt,\n",
    "                                       cv = cv_cnt,\n",
    "                                     )\n",
    "        gs.fit( X, y )\n",
    "\n",
    "        print( '=========================================' )\n",
    "        print( 'model : ', str( model ).split( '(' )[0] )\n",
    "        print( 'best_score : ', gs.best_score_ )\n",
    "\n",
    "#         print( '=================' )\n",
    "#         print( 'best model : ', gs.best_estimator_ )\n",
    "#         print( '=================' )\n",
    "\n",
    "        df_feature_importance = pd.DataFrame( X.columns.values, columns = [ 'features' ] )\n",
    "        df_feature_importance[ 'importance' ] = gs.best_estimator_.feature_importances_ * 100\n",
    "#         print( df_feature_importance.sort( 'importance', ascending = False  ) )    \n",
    "\n",
    "\n",
    "        y_pred = gs.best_estimator_.predict( df_test_merged )    # prediction 수행\n",
    "        df_result[ 'cost' ] = np.expm1( y_pred )\n",
    "\n",
    "        now = time.strftime( '%Y%m%d%H%M%S' )    # 현재시각을 확보\n",
    "        file_timestamp = now[2:4] + now[4:6] + now[6:8] + now[8:10] + now[10:12] + now[12:14]\n",
    "        accuracy = '{0:.2f}%'.format( gs.best_score_ * 100 )    # latitude에 대한 예측률 저장 : 파일명 활용용도임\n",
    "\n",
    "        if bracket == 'Yes' :\n",
    "            df_result.to_csv( path_or_buf = './result.to.submit/' + file_timestamp + '.' + model_name +\n",
    "                              '.' + accuracy + '.result().bracket.Y.csv', sep = ',', index = False )\n",
    "        elif bracket == 'No' :\n",
    "            df_result.to_csv( path_or_buf = './result.to.submit/' + file_timestamp + '.' + model_name +\n",
    "                              '.' + accuracy + '.result().bracket.N.csv', sep = ',', index = False )\n",
    "        elif bracket == 'All' :\n",
    "            df_result.to_csv( path_or_buf = './result.to.submit/Case145s/' + file_timestamp + '.' + model_name +\n",
    "                              '.' + accuracy + '.result().bracket.All.csv', sep = ',', index = False )                \n",
    "\n",
    "\n",
    "        list_features = df_test_merged.columns.values.tolist()    \n",
    "\n",
    "        with open( 'performance.condition.history.txt', 'a' ) as history_file :    # 모델의 이력을 logging\n",
    "            history_file.write( '======================================================================\\n' )\n",
    "            history_file.write( '0. Description :\\n' )\n",
    "            if mode == True :\n",
    "                history_file.write( '\\tmode : Production mode\\n' )\n",
    "            else :\n",
    "                history_file.write( '\\tmode : Test mode\\n' )\n",
    "            history_file.write( '1. File name :\\n\\t' + file_timestamp + '.' + model_name + '.' + accuracy + '\\n' )\n",
    "            history_file.write( '\\t' + str( gs.best_score_ ) + '\\n' )\n",
    "            history_file.write( '2. Model information :\\n\\t' + str( gs.best_estimator_ ) + '\\n' )\n",
    "            history_file.write( '3. Applied features :\\n' )\n",
    "            for feature in list_features :\n",
    "                history_file.write( '\\t' + feature + '\\n' )\n",
    "            history_file.write( '4. Removed features :\\n' )\n",
    "            for feature in list_for_remove :\n",
    "                history_file.write( '\\t' + feature + '\\n' )\n",
    "            history_file.write( '5. Feature importance :\\n\\t' )        \n",
    "            history_file.write( df_feature_importance.sort( 'importance', ascending = False  ).to_string() )\n",
    "            history_file.write( '\\n' )\n",
    "            history_file.close()\n",
    "        rep_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_result.head( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_result_desc = df_result.describe()    # 결과의 overview 확인\n",
    "df_result_desc.drop( 'id', axis = 1, inplace = True )\n",
    "df_result_desc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del( X )\n",
    "del( y )\n",
    "del( df_test )\n",
    "del( df_test_merged )\n",
    "del( df_components )\n",
    "del( df_result_desc )\n",
    "del( df_result )\n",
    "del( df_tube_min_max_quantity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1개 0.5시간\n",
    "- 5개 2.5시간\n",
    "- 10개 5시간\n",
    "- 20개 10시간\n",
    "- 30개 15시간\n",
    "- 40개 20시간 하루\n",
    "- 50개 30시간\n",
    "- 80개 40시간 \n",
    "- 100개 50시간 이틀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
